---
title: "Matrix Algebra - Refresher"
author: "T. Markaryan"
date: "May 11, 2016"
output: 
  pdf_document:
    fig_caption: true
    keep_tex: true
    includes:
      in_header: MatrixLatexLibraries.sty
---

A special value of matrices and Matrix Algebra is that they nable to express many operations arising in mathematics, statistics and many other fields in a clear concise manner. Matrices also enhance the understanding of which apsects generalize to higher dimensions.


#1. Matrices as Transformations

####$R \to R$ 

$$y=ax$$

A single number $\underline{a}$ represents linear transformation.

####$R^2 \to R$ 

$$y=a_1x_1+a_2x_2$$

Vector $\mathbf{A}=(a_1, a_2)$ represents linear transformation.

####$R^2 \to R^2$ 


$$\begin{cases} y_1 = a_{11}x_1 + a_{12}x_2\\ y_2 = a_{21}x_1 + a_{22}x_2 \end{cases}$$

Matrix  $\begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}$ represents linear transformation.

\begin{tcolorbox}[colback=yellow!5,colframe=yellow!40!black,title=Questions]
1. What matrix corresponds to a linear mapping?

2. What matrix corresponds to a linear maping $R^{10} \to R^{20}?$
\end{tcolorbox}

Thus, for linear mappings:

- $R \to R$  | given by numbers

- $R^{n} \to R^{m}$  | given by matrices


So, we can write compactly

 \begin{equation}\label{eqn:matrix}\underline{y}=A\underline{x}\end{equation}
 
 where
 
 $$\underline{y}=\binom{y_1}{y_2},  A = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} , \underline{x}=\binom{x_1}{x_2}$$


Note that equation \eqref{eqn:matrix} looks similar to the linear equation in the case of $R \to R$

In general, for transformation $R^{n} \to R^{m}$, a matrix  $$\begin{pmatrix}A\end{pmatrix}_{m*n}$$ reprezents a linear transformation. 

Viewing matrices as linear transformations can help understand, seemingly strange definition of a product of matrices as well as:

- Addition of matrices

- Multiplication by a scalar

- Inverse 




#2. Notation and Definitions\



\begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
A matrix is an $m*n$ array of numbers with  $\underline{m}$ rows and  $\underline{n}$ columns.
\end{tcolorbox}
\    



Matrices are usually denoted by capital's: A,B, C...


Generic elemment of matrix is written as:

\begin{center}
$\begin{pmatrix}A\end{pmatrix}_{ij}$ or $a_{ij}$, where  $i=1,...,m ; j=1,...n$
\end{center}
\






Examples of Matrices:

  - $\mathbf{1x1}$ marix is a $\underline{scalar}$
  
  - $\mathbf{nx1}$ marix is a $\underline{column vector}$
  
  - $\mathbf{1xn}$ marix is a $\underline{row vector}$
  
  - $\mathbf{nxn}$ marix is a $\underline{square matrix}$


We denote column vectors by $a, b, z...$ and to emphasize that they are vectors we use:\

$\underline{a}, \underline{b}, \underline{z}$.\






$\mathbf{Equality}$ of matrices: $A=B$ 

  - They are the same size
  - $A_{ij}=B_{ij}$
  
  
$\mathbf{Transpose}$ of matrix:

   - If $A$ is a $m*n$ matrix, then $(A')_{ij}$ = $(A)_{ij}$ is a $n$x$m$ matrix.
\


Examples of transpose will be provide.\



$\mathbf{Symmetric}$ $A'=B$ \





#3. Matrices as Transformations


####Addition of Matrices\





\begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
If $A$ and $B$ are $mxn$ matrices then $C=A+B$ is defined as elementwise addition.

$(C)_{ij}$ = $(A)_{ij}$ + $(B)_{ij}$ 	   $\forall$            ${i,j}$
\end{tcolorbox}
\  





$\mathbf{Examples}$ \




a) A= $\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$ 
  B= $\begin{pmatrix} -1 & 1 \\ 1 & -4 \end{pmatrix}$;
  A+B=$\begin{pmatrix} 1+(-1) & 2+1 \\ 3+1 & 4+(-4) \end{pmatrix}$
  \
  
  
  
  
  
  
  
  b)  A= $\begin{pmatrix} 1 & 3 & 5 \\ 2 & 4 & 6\end{pmatrix}$ 
      B= $\begin{pmatrix} -1 & -3 & -5 \\ -2 & -4 & -6\end{pmatrix}$; A+B=$\begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 0\end{pmatrix}$\
      
      
      
      
      
$\textbf{Additive Identities }$\







Matrix  $O_{mxn}$  consisting of all $O's$ is called $\textbf{Null Matrix}$ and serves as the additive identitiy.\




$$A_{mxn}+O_{mxn}=A_{mxn}$$\



-> $\text{\textit{Note: There is an additive identity for each size}}$\






$\textbf{Properties of Matrix Addition}$\





- Commutative: $A+B = B+A$\

- Associative: $A+(B+C) = (A+B)+C$    $\text{\textit{Why?}}$\

- Multiplication by a scallar: If $A = A_{ixj}$ then $B=cA=(c*A_{ixj})$

$\mathbf{Example}$ \







4*$\begin{pmatrix} 1 & 0 & -1 \\ 2 & 1 & 4\end{pmatrix}$ = $\begin{pmatrix} 4 & 0 & -4 \\ 8 & 4 & 16\end{pmatrix}$\




#4. Multiplication of Matrices\



Matrix multiplication may first seem unnatural or strange, but viewing it as a $\textbf{composition}$ of two linear maps makes it completelly clear.\


$\mathbf{Example}$ \

  - One-dimensional case
  

  
  \begin{center}
    \begin{tikzpicture}
      \SetGraphUnit{3}
      \Vertex[L=R]{B}
      \WE[L=R](B){A}
      \EA[L=R](B){C}
      \Edge[label = I](A)(B)
      \Edge[label = II](B)(C)
      \tikzset{EdgeStyle/.append style = {bend left = 50}}
      \Edge[label = III](A)(C)
    \end{tikzpicture}
  \end{center}
  
  \begin{tikzcd}
a\ar[r,"A"]\ar[rr,out=-30,in=210,swap,"C"] & b\ar[r,"B"] & c
\end{tikzcd}
  
  $I$: $y=\alpha*x$\
  
  $II$:  $\zeta=\beta*y$\
  
  $III$:$\zeta=\beta*\alpha*x$
  
  
  
  
   -  $\mathbbm{R^2}$  case
   
   
  
  $$\mathbbm{R^2}\xrightarrow{--1--}\mathbbm{R^2}\xrightarrow{--2--}\mathbbm{R^2}$$
  $$\overrightarrow{---3---}$$\
  
  
  1:$\begin{cases} y_1 = x_1 + x_2\\ y_2 = x_1 - x_2 \end{cases}$\
  
  and, 
  
  \begin{equation}\label{eqn:matrix}\underline{y}=A\underline{x}\end{equation}
  
   $$A = \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}$$
  
  
  
  2:  $\begin{cases} \zeta_1= y_1 + 2y_2\\ \zeta_2 = y_1 - y_2 \end{cases}$\
  
  
  
  and,
  
  \begin{equation}\label{eqn:matrix}\underline{\zeta}=B\underline{y}\end{equation}
  
   $$B = \begin{pmatrix} 1 & 2 \\ 1 & -1 \end{pmatrix}$$\
   
   
   
  
  3:$\begin{cases} \zeta_1= x_1 + x_2 + 2x_1 - 2x_2 \\ \zeta_2 = x_1 + x_2 - x_1 + x_2 \end{cases}$ 	$\Longleftrightarrow$   $\begin{cases} \zeta_1= 3x_1 - x_2\\ \zeta_2 = 2x_2 \end{cases}$\
  
  
  thus, 
  
  
  
  \begin{equation}\label{eqn:matrix}\underline{\zeta}=C\underline{x}\end{equation}
  
   $$C = \begin{pmatrix} 3 & -1 \\ 0 & 2 \end{pmatrix}$$\
   
   
So it is natural to define $BA=C$, and we can easily verify that:

 $$BA = \begin{pmatrix} 1 & 2 \\ 1 & -1 \end{pmatrix}\begin{pmatrix} 1 & 1\\ 1 & -1 \end{pmatrix}=\begin{pmatrix} 3 & -1 \\ 0 & 2 \end{pmatrix}$$\
 
 
 
 Identifying multiplication as a composition should make it natural that matrix mulitplication is only defined for matrices:\
 
 
 $$\begin{pmatrix}A\end{pmatrix}_{m*n} \begin{pmatrix}B\end{pmatrix}_{n*p}$$
 
 $$n=m$$\
 
 -> Dimensions must conform before $A*B$ can be defined.\
 
   $$\mathbbm{R_p}\xrightarrow{--B--}\mathbbm{R_n}\xrightarrow{--A--}\mathbbm{R_m}$$
  $$\overrightarrow{---AB---}$$\
  
  
  \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
Let $\begin{pmatrix}A\end{pmatrix}_{m*n}$ and $\begin{pmatrix}B\end{pmatrix}_{n*p}$  then $C=AB$ is defined
as $\begin{pmatrix}C\end{pmatrix}_{i*j} = \sum\limits_{k=1}^n \mathcal A_{ik}B_{kj}$ ; i=1,...,m; j=1,...,p
\end{tcolorbox}
\ 



$\mathbf{Examples}$ \


a)  $\begin{pmatrix} 3 & 1 & 10 \\ 2 & 2 & 8 \end{pmatrix}$ 
   $\begin{pmatrix} 60 \\ 80 \\ 40 \end{pmatrix}$ = $\begin{pmatrix} 660 \\ 600  \end{pmatrix}$\
   
   
   
  -> The matrices are represented by the dimensions as follows: 2x3, 3x1 and 2x1.
  \
  
  
b)  $\begin{pmatrix} 1 & 2 \\ 3 & 4  \end{pmatrix}$ 
   $\begin{pmatrix} 0 & -1 \\ 1 & -1  \end{pmatrix}$ = $\begin{pmatrix} 2 & -3 \\ 4 & -7  \end{pmatrix}$\
   
   
   
  -> The matrices are represented by the dimensions as follows: 2x2, 2x2 and 2x2.
  \
  
    
c)  $\begin{pmatrix} 0 & -1 \\ 1 & -1  \end{pmatrix}$ 
   $\begin{pmatrix} 1 & 2 \\ 3 & 4  \end{pmatrix}$ = $\begin{pmatrix} -3 & -4 \\ -2 & -2  \end{pmatrix}$\
   
   
   
d)  Inner Product\

      $\begin{pmatrix} 2 & 2 & 8 \end{pmatrix}$ 
   $\begin{pmatrix} 50 \\ 100 \\ 30  \end{pmatrix}$ = $540$\
   
-> The matrices are represented by the dimensions as follows: 1x3, 3x1 and 1x1.
  \
  
  
More generally, if $\begin{pmatrix} x_i \\ .\\ .\\.\\x_n  \end{pmatrix}$ $\begin{pmatrix} y_i \\ .\\ .\\.\\y_n  \end{pmatrix}$,  then  $X'Y=Y'X = \sum\limits_{i=1}^n \mathcal X_{i}Y_{i}$


e)  Outer Product\


    $\begin{pmatrix} 50 \\ 100 \\ 30  \end{pmatrix}$ $\begin{pmatrix} 2 & 2 & 8 \end{pmatrix}$ = $\begin{pmatrix} 100 & 100 & 400 \\ 200 & 200 & 800 \\ 60 & 60 & 240 \end{pmatrix}$\
   
   
   
   
-> The matrices are represented by the dimensions as follows: 3x1, 1x3 and 3x3.
  \
 
 
 
-> $\text{\textit{Note: b) $\neq$ c) which means that $\textbf{matrix mulitplication}$ is  $\textbf{NOT}$ commutative. Examples d) and e) confirm that.}}$\
   
  \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition (Linear Forms)]
If if $\underline{x}=\begin{pmatrix} x_i \\ .\\ .\\.\\x_n  \end{pmatrix}$ is a vector of variables $x_1, ...x_n$ and $\underline{a}=if \begin{pmatrix} a_i \\ .\\ .\\.\\a_n  \end{pmatrix}$ is a vector of constants than $a'x=a_1x_1+a_2x_2+...a_nx_n$ is called a $\textbf{linear form}$ in x.



\end{tcolorbox}\







It is easy to show that:

$$\frac{d}{dx}(a'x)=\underline{a}$$\





 \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition (Length of a vector)]
If if $\underline{x}=\begin{pmatrix} x_i \\ .\\ .\\.\\x_n  \end{pmatrix}$ $\in \mathbbm{R^n}$ then, $\|x\|$=$\sqrt{x'x}$=$\sqrt{\sum\limits_{i=1}^n x^2_i}$, if $\|x\|$=1, then x has an unit length.




\end{tcolorbox}\




$\textbf{Quadratic Forms}$.

If A is symmetric then $\underline{x'}A\underline{x}$ is called a $\textbf{quadratic form}$\


-> $\text{\textit{Note: $x'Ax$ ia a scalar.}}$\



$\mathbf{Examples}$ \



$\begin{pmatrix} x_1 & x_2 \end{pmatrix}$ 
   $\begin{pmatrix} 1 & 3 \\ 3 & 2 \end{pmatrix}$ $\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$ = ${x^{2}_1 + 6x_1x_2 + 2x^{2}_2}$\ 
                                  ->$\text{\text{(quadratic form in 2 variables)}}$\
                                  
                                  
                                  
                                  
                                  
                                  
                                  
                                  
  -In 1-dimensional case: $x*a*x = ax^2$ & $\frac{d}{dx}(x*a*x)=2ax$\


  -In higher dimensions: $\frac{d}{dx}(\underline{x'}*A*\underline{x})=2A\underline{x}$ -> $\text{\textit{Show it!}}$\
  
  
  
  
  
$\textbf{Definite and semi-definite matrices.}$\
  
  
  
  \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
A symmetric matrix $\underline{A}$ is said to be $\textbf{positive definite}$ if $\underline{x'}A\underline{x}$  $>$ for all $\underline{x}\neq0$



\end{tcolorbox}\



  -Positive semi-definite is defined by changing $">"$ to $"\geq"$.
  
  -Negative definite and negative semi-definite are defined using $"<"$ and $"\leq"$, respectively.\
  
  
  
  
$\mathbf{Example}$\



  -Variance-Covariance Matrices are Positive Semi-Definite.\
  
  
  
  
  
$\mathbf{Example}$\


a) Show that:\

$A=\begin{pmatrix} 2 & -2 \\ -2 & 2 \end{pmatrix}$ is positive semi-definite.\


b) Show that:\

$B=\begin{pmatrix} 4 & -2 & 0\\ -2 & 4 & -2 \\ 0 & -2 & 4\end{pmatrix}$ is positive definite.\


c) Show that:\

$C=\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}$ is indefinite.\



  
$\textbf{Properties of Matrix Multiplication.}$\



  -1. Associative:\
  
    $A(BC)=(AB)C$\
  
  -2. Distributive:\
  
    $A(B+C)=AB+AC$\
    
    $(B+C)A=BA+CA$\
    
  -3. NOT-Commutative\
  
  -4. Identity Matrix\
  
  $I_{nxn}$=$\begin{bmatrix} 1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 1 \end{bmatrix}$ serves as multiplicative identity for square matrices for $\forall$ $A_{nxn}$
  
  $$I_{nxn}A_{nxn}=A_{nxn}I_{nxn}=A_{nxn}$$\
  
  
  

-> $\text{\textit{Note:In case of identity matrix product does not commute!}}$\


-v5. For any matrix $A_{mxn}$\

$$A_{mxn}O_{nxp}=O_{mxp}$$\

$$(AB)'=B'A'$$
$$(ABC)'=C'B'A'$$\



#5. Inverse of Matrices\




  
  \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
  If for a squarmatrix $A_{nxn}$ $\exists$  a matrix $B_{nxn}$ sit.\
  
  (*) $AB = BA = I_{nxn}$.  Than $B$ is calles an $\underline{inverse}$ of $A$ and denoted $A^-1$. If $A$ has an inverse, it is called $\underline{non-singular}$.
  

\end{tcolorbox}\


  -It turns out that one needs to check only one of the 2 conditions in (*).\
  
  -The inverse if it exists is $\underline{unique}$.\
  
  
  
  
$\mathbf{Example}$\

a)$A=\begin{pmatrix} 2 & -1 \\ -1 & 1  \end{pmatrix}$ then $A^{-1}=\begin{pmatrix} 1 & 1 \\ 1 & 2  \end{pmatrix}$  -> check directly.\


b)Show that $A=\begin{pmatrix} 1 & 3 \\ 2 & 6  \end{pmatrix}$ does not have an inverse.\


c)Show that if $A=\begin{pmatrix} a & b \\ c & d  \end{pmatrix}$ than $A^{-1}
=\frac{1}{ad-bc}\begin{pmatrix} d & -b \\ -c & a  \end{pmatrix}$ iff ad-bc $\neq$ 0.\


####Properties of Matrix Inverse\

  a) $(A')^{-1}=(A^{-1})'$\
  
  b) $(A^{-1})^{-1}=A$\
  
  c) $(AB)^{-1}=B^{-1}A^{-1}$\
  
  
####Proof of c)\

Need to show that:\

$$(AB)(B^{-1}A^{-1})= I$$

Now:

  
\begin{align}
    (AB)(B^{-1}A^{-1}) &=A(BB^{-1})A^{-1}&& \text{(Associative}  \text{)}\\
     &=AIA^{-1} && BB^{-1}=I \\
     &=AA^{-1}   && BB^{-1}=I \\
     &=I &&\text{(Definition of Inverse}  \text{)}
\end{align}

\


c) Also extends to more than 2 matrices. For example:\
$$(ABC)^{-1}=C^{-1}B^{-1}A^{-1}$$

----> Show it!\

  -Armed with definition of matrix inverse we can cast system of linear equations and solution in the matrix form.\
  
  
 (*) $$\begin{cases} x_1 + 2x_2 +x_3= 4\\ 4x_2 + 3x_3 = 3\\3x_1 + 6x_2 = -3 \end{cases}$$\
  
  Denote:\
  
  
$A=\begin{pmatrix} 1 & 2 & 1\\ 0 & 4 & 3 \\ 3 & 6 & 0\end{pmatrix}$, $b=\begin{pmatrix}  4\\ 3 \\-3\end{pmatrix}$ , $x=\begin{pmatrix}  x_1\\ x_2 \\x_3\end{pmatrix}$\




  We can write (*) as:\
  
  (**) $$A\underline{x}=\underline{b}$$
  
  Also, if (*) has an unique solution then:
  
  $$\underline{x}=A^{-1}b$$
  
  Which is how we solve in case of $\mathbbm{R^1}$.\
  
  
  - We will return and solve this equation after introducing column reduction.
  
  - As we have already seen, not every square matrix has an inverse. A <=> condition for existence of the inverse is for the columns of the matrix to be
 $\textbf{LINEARLY INDEPENDENT}$.\
 
 
 
####Linear Independence\

- Two vectors of same size:\

$a=\begin{pmatrix} a_1 \\ .\\ .\\.\\a_n  \end{pmatrix}$ & $b=\begin{pmatrix} b_1 \\ .\\ .\\.\\b_n  \end{pmatrix}$\


Are called $\textbf{LINEARLY INDEPENDENT}$ if for $\forall$   $\lambda_1$, $\lambda_2$, $\in$ $R$ siti $\lambda^{2}_1 + \lambda^{2}_2 > 0$\

$$\lambda_1a + \lambda_2b \neq \underline{0}$$, \


Otherwise there are called $\textbf{LINEARLY DEPENDENT}$.\



####EXAMPLES\


a) $a=\begin{pmatrix}  1\\ 2 \end{pmatrix}$ and $b=\begin{pmatrix}  4\\ 8 \end{pmatrix}$ --> Ale Linearly Dependent\

choose $\lambda_1 = -4$ & $\lambda_2 = 1$\


b) $a=\begin{pmatrix}  1\\ 0 \end{pmatrix}$ and $b=\begin{pmatrix}  0\\ 1 \end{pmatrix}$ --> Ale Linearly Independent\

choose $\lambda_1a +\lambda_2b= 0$\

$$<=>$$

$$\begin{cases} \lambda_1= 0\\ \lambda_2 = 0 \end{cases}$$\


The concept extends to .... $\forall$ number of vectors.
\







Set of vectors $\underline{x_1},...\underline{x_i}$,  for  $\underline{x_i} \in$ $R^{n}$\


is called $\textbf{LINEARLY INDEPENDENT}$ if for $\forall$  $\lambda_1,...,\lambda_p$ $\in$ R, $\lambda^2_1+\lambda^2_2+\lambda^2_3+...+\lambda^2_p > 0$\


$$\sum\limits_{k=1}^p \lambda_k \underline{x_k} \neq \underline{0}$$


Otherwise , they are linearly dependent.\


####EXAMPLE\

The Vectors:\

$\begin{pmatrix}  0\\ 3\\2 \end{pmatrix}$,  $\begin{pmatrix}  1\\ -1\\-3 \end{pmatrix}$ , $\begin{pmatrix}  2\\ 2\\-1 \end{pmatrix}$ and  $\begin{pmatrix}  1\\ 13\\11 \end{pmatrix}$ are Linearly Dependent.




####RANK OF MATRIX\


  
  
  
  \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
Rank of a matrix is the largest number of Linear Independent or Rows. 



\end{tcolorbox}\


  
  \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
A square matrix $A_{nxn}$ is called $\textbf{Full-Rank}$ if rank $(A)=n$.



\end{tcolorbox}
\




Full rank square matrices have inverses ..... is also true.


####EXAMPLE(REVISIT)\

$$A=\begin{pmatrix} 1 & 3 \\ 2 & 6 \end{pmatrix}$$


-> $\text{\textit{Note that column linearly dependent.}}$\

$-3*\begin{pmatrix} 1  \\ 2 \end{pmatrix} + 1*\begin{pmatrix} 3  \\ 6 \end{pmatrix}$ = 0\



  \begin{tcolorbox}[colback=green!5,colframe=blue!40!black,title=Theorem]
If A is non-singular then Rank(AB)=Rank(B)=Rank(BA). 



\end{tcolorbox}
\



#6. Determinant of a  Matrix\


  -You may remember determinants from your linear algebra courses mean applying $\textbf{Crame's Rule }$ to solving $\textbf{systems of linear equations}$, also $\textbf{Jacobians}$.
  
  
  -Determinants are defined for $\textbf{square matrices}$.
  
  -Geometrically, the absolute value of determinant is the volume of the parallelpiped spanned by its column vectors.
  
  -The determinant of matrix $A$ is denoted by $|A|$.
  
  -The determinant of a 2x2 is $\begin{pmatrix} a & b \\ c & d \end{pmatrix}$=$ad-bc$
  
  -Direct calculation of determinants for matrices of larger sizes is combersome and usually some decomposition method is used. Calculating 3x3 is still ok:\
  
  $$\begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23}\\a_{31} & a_{32} & a_{33} \end{pmatrix}=a_{11}a_{22}a_{33}+a_{21}a_{32}a_{13}+a_{12}a_{23}a_{31}-a_{13}a_{22}a_{31}-a_{12}a_{21}a_{33}-a_{11}a_{32}a_{23}$$ \
  
  
####PROPERTIES OF DETERMINANT\

a) $|A|=|A'|$

b)  $|AB|=|A||B|$

c) $|A|=0$ <=> $A$ is singular

d)  If $\exists A^{-1}$ then $|A'|$=$\frac{1}{|A|}$

e) $|kA|= k^{n}|A|$, if $A_{nxn}$

f) Interchanging 2 rows (columns) changes the sign of determinant.

g) Multiplying of an entire row (column) by a constant multiplies the determimant by that constant.

h) Adding a scalar multiple of one row (column) to another row (column) does not change the value of the determinant.

i) If matrix is triangular , determinant is a product of diagonal elements.


####EXAMPLE\

 

  1. A=$\begin{pmatrix} 1 & 2 \\ 1 & d-1 \end{pmatrix}$, then $|A|$ = $1*(-1)-1*2=-1-2=-3$\
  
  
  2. $\begin{pmatrix} 1 & -1 \\ 1 & 2 \end{pmatrix}$ = $1*2-1*(-1)=3$  -->(f)\
  
  3. $|2*\begin{pmatrix} 1 & -1 \\ 1 & 2 \end{pmatrix}|$ = $\begin{pmatrix} 2 & 4 \\ 2 & -2 \end{pmatrix}$ = $-4-8=-12=2^2*(-3)$ -->(e)\
  
  4.$\begin{pmatrix} 5 & 2 \\ 5 & -1 \end{pmatrix}$ = $-5-10=-15=5*(-3)$ -->(g)\
  
  5. Let us multiply col 1 by -2 and add to col 2\
  
  $\begin{pmatrix} 1 & 0 \\ 1 & -3 \end{pmatrix}$ = $-3$ same as |A| -->(h)\
  
  
  
#7. PATRITIONED MATRICES\


We can partition matrices into $\textbf{blocks}$.\


####EXAMPLE\




$A=\left[\begin{array}{c|c}\begin{matrix} 1 & 2 \\ 3 & 6 \end{matrix} & \begin{matrix} 0 & 4 \\ 8 & 0 \end{matrix} \\ \hline \begin{matrix} 1 & 2 \end{matrix} & \begin{matrix} 0 & 1\end{matrix} \end{array} \right]=\left[\begin{array}{c|c}A_{11} & A_{12} \\ \hline A_{21} & A_{22} \end{array} \right]$


$A_{11}=\begin{pmatrix} 1 & 2 \\ 3 & 6 \end{pmatrix}$, $A_{12}=\begin{pmatrix} 0 & 4 \\ 8 & 0 \end{pmatrix}$, $A_{21}= \begin{pmatrix} 1 & 2\end{pmatrix}$, $A_{22}= \begin{pmatrix} 0 & 1\end{pmatrix}$\


As long as dimensions conform, matrix addition , multiplication, transposition work the same way. That is the power of PARTITIONING.\



####EXAMPLE\


$\underline{b}=\left[\begin{array}{c|c} 2 \\ 0 \\ \hline -1 \\ 2 \end{array}\right]$ =
$\begin{pmatrix} b_1 \\ b_2\end{pmatrix}$\


Let's do multiplication with partitioned matrices:\



$A\underline{b}=\left[\begin{array}{c|c}A_{11} & A_{12} \\ \hline A_{21} & A_{22} \end{array} \right]\begin{pmatrix} b_1 \\ b_2\end{pmatrix}$ = $\begin{pmatrix} A_{11}b_1 +  A_{12}b_2\\ A_{21}b_1 +  A_{22}b_2\end{pmatrix}$, where:\



$A_{11}b_1=\begin{pmatrix} 2 \\ 6\end{pmatrix}$ ; $A_{12}b_2=\begin{pmatrix} 8 \\ -8\end{pmatrix}$ ; $A_{21}b_1=(2)$;  $A_{22}b_2=(2)$; \

So: $AB = \begin{pmatrix} 10 \\ -2 \\ 4\end{pmatrix}$\

Of course we get the same answer by performing multiplication on original matrices. (DO IT!)\






####EXAMPLE\

If R&S are non-singular show that : $\begin{pmatrix} R & 0 \\ L & S\end{pmatrix}^{-1}$ = $\begin{pmatrix} R^{-1} & 0 \\ -S^{-1}LR^{-1} & S^{-1}\end{pmatrix}$\


PROOF --->  Provide.


     
      
####EXAMPLE\ 

If A is a block diagonal matrix:\

  $A$=$\begin{bmatrix} A_{11} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & A_{nn} \end{bmatrix}$\
  
Then\

$det(A)=det(A_{11})*det(A_{11})*det(A_{22})...det(A_{nn})$

Let:\

$A =\begin{pmatrix} 2 & 0 & 0\\ 0 & 4 & 5 \\ 0 & 6 & 7 \end{pmatrix}$\



Find det(A) --->  Provide solution.



#7. SOME SPECIAL MATRICES\

  - SYMMETRIC MATRICES\
  
  
    Square matrix A, $A^{T}=A$\
    
    Variance - Covariance matrices are symmetric.\
    
  - ORTHOGONAL MATRICES (RIGID TRANSFORMATION)\
  
    
    
    Two vectors $\underline{x}$ & $\underline{y}$ are called $\textbf{ORTHOGONAL}$ if $\underline{x^{T}}$ $\underline{y}$ = 0 (Dot Product)\
    
    
    Two vectors $\underline{x}$ & $\underline{y}$ are called $\textbf{ORTHOGONAL}$ if:\
    
    1) They are orthogonal and
    
    2) $\underline{x^{T}}$ $\underline{x}$ = 1 ; $\underline{y^{T}}$ $\underline{y}$ = 1 (Unit Length)\
    
    
    
    
  - A square matrix Q is called $\textbf{ORTHOGONAL}$ if its columns are ORTHOGONAL.\
  
  
  

####EXAMPLE\

Is $A =\begin{pmatrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\end{pmatrix}$ orthonormal? Show it.\


  - Properties of ORTHOGONAL Matrices.\
  
    i) $Q^{T}Q=I$\
    
    ii) $Q^{-1}=Q^{T}$\
    
    iii) |delta| = 1 ---> det is +/- 1\
    
  - Idempotent Matrices.\
  
  
  
      A is idempotent <--> $A^{2}=A$ 
      
      If A is idempotent than $(I - H)$ is too. 
      
      
####EXAMPLE\

The hat matrix from OLS

$H=X(X'X)^{-1}X'$ is idempotent.   Show it!



##PERMUTATION MATRICES\


$\textbf{P}$ is called a $\textbf{Permutation Matrix}$ if it can be obtained from identity matrix by permuting columns.\


 -Properties:
 
  a) Permutation Matrices are  $\textbf{orthogonal}$. 
     Their determinant is +/- 1.\
      
     Also if $|A| = a$ and $\textbf{P}$ is a Permutation Matrix , then $|AP|$ = $|PA|$ = +/- a \
     
     
     
##DIAGONAL MATRICES\

If only non zero elements are on diagonal:\

 $I_{nxn}$=$\begin{bmatrix} a_{11} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & a_{nn} \end{bmatrix}$\
 
Properties:

  a) Determinant = $a_{11}*a_{22}*...*a_{nn}$\
  
  b)  $\prod_{i=1}^{n}a_{ii} \neq 0$ <=> Full Rank\
  
  
  
  \begin{equation}
  \left(
    \begin{array}{*5{c}}
     x & x & x & x & x \\
     0 & x & x & x & x \\
     0 &  0 & x & x & x \\
     0 &  0 &  0 & x & x \\
     0 &  0 &  0 &  0 & x \\
  \end{array}\right)
\end{equation}

---> UPPER TRIANGULAR


\begin{equation}
  \left(
    \begin{array}{*5{c}}
     0 & 0 & 0 & 0 & 0 \\
     x & 0 & 0 & 0 & 0 \\
     x &  x & 0 & 0 & 0 \\
     x &  x &  x & 0 & 0 \\
     x &  x &  x &  x & 0 \\
  \end{array}\right)
\end{equation}


---> LOWER TRIANGULAR\


Properties ( the same as for diagonal matrices):

  a) Determinant = $a_{11}*a_{22}*...*a_{nn}$\
  
  b)  $\prod_{i=1}^{n}a_{ii} \neq 0$ <=> Full Rank\
  
  
#8. COLUMN REDUCTION\

Column reduction of matrices is used for various purposes, including:\


  -Determinig if matrix is full rank\
  
  -Calculating inverse.\
  
  -Calculating the determinant.\
  
  -Solving system of linear equations.\
  
  
Column reduction involves 3 types of so called elementry column operations.\


Each  elementry column operation is equivalent to multiplying the matrix from the right by a special type of matrix.\




$\underline{Notes:}$


  -This should remind us those operations that we perform on equations which preserve certain properties (most commonly, solutions)
  
  -One could also perfrom row reduction in which case one pre-multiplies vs. post-multiplication.\
  
  
###ELEMENTARY COLUMN OPERATIONS\

$\bold{1}$. SWITCH  2 COLUMNS: $\underline{i}$ & $\underline{j}$\

  -Multiply from right by the permutate matrix that is obtained from the identity matrix by swapping its $\underline{i^{th}}$ & $\underline{j^{th}}$ columns.\
  
  


####EXAMPLE\

$A =\begin{pmatrix} 2 & 6 & 1\\ 5 & -5 & -2 \\ -2 & 3 & 1 \end{pmatrix}$\


Swap columns 2<--->3\


Let: $P =\begin{pmatrix} 1 & 0 & 0\\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{pmatrix}$\




$AP$=$\begin{pmatrix} 2 & 6 & 1\\ 5 & -5 & -2 \\ -2 & 3 & 1 \end{pmatrix}\begin{pmatrix} 1 & 0 & 0\\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{pmatrix}$ = $\begin{pmatrix} 2 & 5 & -2\\ 1 & -2 & 1 \\ 6 & -5 & 3 \end{pmatrix}$\


$\underline{Notes:}$\


$P^{-1}=P^{1}$, $|P|$= -1\



\begin{tcolorbox}[colback=yellow!5,colframe=yellow!40!black,title=Question]
1. What features are preserved?

(1.Rank, 2. Determinant gets mulitplied by -1)

\end{tcolorbox}\






$\bold{2}$. MULTIPLY $\underline{j^{th}}$ COLUMN BY A SCALAR $\underline{n}$\


  -Multiply from the right by the identity matrix, $(jj)^{th}$
element of which is replaced by  $\underline{n}$\



####EXAMPLE\



$A =\begin{pmatrix} 2 & 6 & 1\\ 5 & -5 & -2 \\ -2 & 3 & 1 \end{pmatrix}$\



Multiply $3^{rd}$ column by $\underline{4}$\



Let: $N =\begin{pmatrix} 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 4 \end{pmatrix}$\



$AN$=$\begin{pmatrix} 2 & 6 & 1\\ 5 & -5 & -2 \\ -2 & 3 & 1 \end{pmatrix}\begin{pmatrix} 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 4 \end{pmatrix}$ = $\begin{pmatrix} 2 & 6 & 4\\ 5 & -5 & -8 \\ -2 & 3 & 4 \end{pmatrix}$\




\begin{tcolorbox}[colback=yellow!5,colframe=yellow!40!black,title=Question]
1. What features are preserved?

(1.Rank, 2. Determinant gets mulitplied by $\underline{n}$)

\end{tcolorbox}\



$\underline{Notes:}$\


$N^{-1} =\begin{pmatrix} 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1/4 \end{pmatrix}$; $|N|= n$\



$\bold{3}$. MULTIPLY COLUMN $\underline{j}$ BY COLUMN BY A SCALAR $\underline{n}$ AND ADD TO COLUMN $\underline{i}$\


  -Multiply from the right by the matrix that is obtained from identity by multiplying its column $\underline{j}$ by $\underline{n}$ and adding to and replacing column $\underline{i}$.\
  
  
  
  
####EXAMPLE\

$A =\begin{pmatrix} 2 & 6 & 1\\ 5 & -5 & -2 \\ -2 & 3 & 1 \end{pmatrix}$\


  -Multiply $1^{st}$ column by $\underline{-3}$ and add to column $\underline{3}$\
  
  
Let: $L =\begin{pmatrix} 1 & -3 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}$\



$Al$=$\begin{pmatrix} 2 & 6 & 1\\ 5 & -5 & -2 \\ -2 & 3 & 1 \end{pmatrix}\begin{pmatrix} 1 & -3 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 4 \end{pmatrix}$ = $\begin{pmatrix} 2 & 5 & -2\\ 0 & -20 & 9 \\ 1 & -2 & 1 \end{pmatrix}$\



$\underline{Notes:}$\



$L^{-1} =\begin{pmatrix} 1 & 3 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}$ = $L(-m)$\

$|L|$=1;\






  -Significance of elementary column operations is that they perserve:
  
  1) The Rank
    
  2) Solution set of system of linear equations
  
  
  -Also, all three types of matrices that are used to perform elementary column operations have inverses. $\textbf{Therefore their product has an inverse.}$\
  
  
  
  -Some of the elementary column operations alter the determinant by a factor of $\bold{-1}$ or $\bold{-n}$. If one keeps track of theses operations, one can still use column reduction to calculate determinants.\
  
  
  
The goal of column operations is to create zeros in critical positions of the matrix, that will then allow to calcuate:\

  * Rank\
  * Inverse\
  * Eigenvalues\
  * Determinant\
  * Etc.\
  
  
  

####EXAMPLE\


Find the rank and the determinant of:\


$A =\begin{pmatrix} 2 & 6 & 1\\ 5 & -5 & -2 \\ -2 & 3 & 1 \end{pmatrix}$\


We will do column reduction:\

\hspace{4 cm}
\begingroup
 
\begin{tabular}{ c c c c c c }
$\bold{Step 1}$ &  & $\bold{Step 2}$  &  & $\bold{Step 3}$ \\
$\begin{vmatrix} 2 & 6 & 1\\ 5 & -5 & -2 \\ -2 & 3 & 1 \end{vmatrix}$ &  & $\begin{vmatrix} 2 & 0 & 1\\ 5 & -20 & -2 \\ -2 & 9 & 1 \end{vmatrix}$ &  & $\begin{vmatrix} 2 & 0 & 2\\ 5 & -20 & -4 \\ -2 & 9 & 2 \end{vmatrix}$ \\  
 & $\xrightarrow{-3*C1 + C2}$ &  & $\xrightarrow{2*C3}$ & & $\xrightarrow{-1*C1+C3}$\\
$\begin{vmatrix} 1 &   0 &   0\\ 0 &   1 &   0 \\ 0 &   0 &   1 \end{vmatrix}$ &  & $\begin{vmatrix} 1 &   -3 &   0\\ 0 &   1 &   0 \\ 0 &   0 &   1 \end{vmatrix}$ &  & $\begin{vmatrix} 1 &   -3 &   0\\ 0 &   1 &   0 \\ 0 &   0 &   2 \end{vmatrix}$\\
 &  &  \\
\end{tabular}
\endgroup

\hspace{3cm}

\begingroup
 
\begin{tabular}{ c c c c c }
 $\bold{Step 4}$ &  & $\bold{Step 5}$  &  & $\bold{Step 6}$ \\
$\begin{vmatrix} 2 & 0 & 0\\ 5 & -20 & -9 \\ -2 & 9 & 4 \end{vmatrix}$ &  & $\begin{vmatrix} 2 & 0 & 0\\ 5 & -20 & -180 \\ -2 & 9 & 80 \end{vmatrix}$ &  &
$\begin{vmatrix} 2 & 0 & 0\\ 5 & -20 & 0 \\ -2 & 9 & -1 \end{vmatrix}$\\
 & $\xrightarrow{20*C3}$ &  & $\xrightarrow{-9*C2 + C3}$ & \\ 
 $\begin{vmatrix} 1 &   -3 &   -1\\ 0 &   1 &   0 \\ 0 &   0 &   2 \end{vmatrix}$ &  &
$\begin{vmatrix} 1 &   -3 &   -20\\ 0 &   1 &   0 \\ 0 &   0 &   40 \end{vmatrix}$ & &
$\begin{vmatrix} 1 &   -3 &  7\\ 0 &   1 &   -9 \\ 0 &   0 &   40 \end{vmatrix}$\\
 &  &  \\
\end{tabular}
\endgroup


Rank(A) = 3\

Det(A) = 2*(-20)*(-1):(2*20) = 1\


####EXAMPLE CONTINOUS\

Find $A^{-1}$\

We continue column reduction working backwords from the last column. The objective is to get 0's at all non diagonal locations.\



--->-9*Col3 + Col2\



\begingroup
 
\begin{tabular}{ c c c c c c}
$\bold{Step 7}$ &  & $\bold{Step 8}$  &  & $\bold{Step 9}$ \\
$\begin{vmatrix} 2 & 0 & 0\\ 5 & -20 & 0 \\ -2 & 0 & -1 \end{vmatrix}$ &  & $\begin{vmatrix} 2 & 0 & 0\\ 5 & -20 & 0 \\ 0 & 0 & -1 \end{vmatrix}$ &  &
$\begin{vmatrix} 8 & 0 & 0\\ 20 & -20 & 0 \\ 0 & 0 & -1 \end{vmatrix}$\\
 & $\xrightarrow{-2*C3 +C1}$ &  & $\xrightarrow{4*C1}$ & & $\xrightarrow{C2 + C1}$ \\ 
$\begin{vmatrix} 1 &   60 &   7\\ 0 &   -80 &   -9 \\ 0 &   360 &   40 \end{vmatrix}$ &  &
$\begin{vmatrix} -13 &  60 &   7\\ 18 &   -80 &   -9\\ -80 & 360 &   40 \end{vmatrix}$ & &
$\begin{vmatrix} -52 &   60 &  7\\ 72 &   -80 &   -9 \\ -320 & 360 &   40 \end{vmatrix}$\\
 &  &  \\
\end{tabular}
\endgroup

\hspace{3cm}


\begingroup
 
\begin{tabular}{ c c c c c c c }
$\bold{Step 10}$ &  & $\bold{Step 11}$  &  & $\bold{Step 12}$ & & $\bold{Step 13}$ \\
$\begin{vmatrix} 8 & 0 & 0\\ 0 & -20 & 0 \\ 0 & 0 & -1 \end{vmatrix}$ &  & $\begin{vmatrix} 1 & 0 & 0\\ 0 & -20 & 0 \\ 0 & 0 & -1 \end{vmatrix}$ &  &
$\begin{vmatrix} 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & -1 \end{vmatrix}$ &  &
$\begin{vmatrix} 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{vmatrix}$\\
 & $\xrightarrow{1/8 * C1}$ &  & $\xrightarrow{-1/20 *C2}$ & & $\xrightarrow{-1*C3}$ \\ 
$\begin{vmatrix} 8 &   60 &  7\\ -8 & -80 &   -9 \\ 40 &   360 &   40 \end{vmatrix}$ &  &
$\begin{vmatrix} 1 &  60 &   7\\ -1 & -80 &   -9\\ 5 & 360 &  40 \end{vmatrix}$ & &
$\begin{vmatrix} 1 &  -3 &   7\\ -1 &   4 &   -9\\ 5 & -18 &   40 \end{vmatrix}$ & &
$\begin{vmatrix} 1 &  -3 &  -7\\ -1 &   4 &    9 \\ 5 & -18 & -40 \end{vmatrix}$\\
 &  &  \\
\end{tabular}
\endgroup

\hspace{2cm}


Thus:\

\hspace{2cm}

$$A^{-1}=\begin{vmatrix} 1 &   -3 &   -7\\ -1 &   4 &   9 \\ 5 &   -18 &   -40 \end{vmatrix}$$\

\newpage

####EXAMPLE\



Solve:

$$\begin{cases} 2x_1 + 6x_2 +x_3 = 0\\ 5x_1 - 5x_2 - 2x_3 = -1\\-2x_1 + 3x_2 +x_3 = 2 \end{cases}$$\




Denote: $x=\begin{pmatrix} x_1\\ x_2\\ x_3 \end{pmatrix}$, $b=\begin{pmatrix} 0\\ -1\\ 2 \end{pmatrix}$, $A=\begin{pmatrix} 2 & 6 & 1\\ 5 & -5 & -2 \\ -2 & 3 & 1 \end{pmatrix}$\





Solution will be: $x = A^{-1}b=\begin{pmatrix} 1 &   -3 &   -7\\ -1 &   4 &   9 \\ 5 &   -18 &   -40 \end{pmatrix}\begin{pmatrix} 0\\ -1\\ 2 \end{pmatrix}=\begin{pmatrix} -11\\ 14\\ -62 \end{pmatrix}$\







####EXAMPLE\



Solve the system of equations by finding $A^{-1}$. Example from page ...\:

  
  
 (*) $$\begin{cases} x_1 + 2x_2 +x_3= 4\\ 4x_2 + 3x_3 = 3\\3x_1 + 6x_2 = -3 \end{cases}$$\
  
Denote:$A=\begin{pmatrix} 1 & 2 & 1\\ 0 & 4 & 3 \\ 3 & 6 & 0\end{pmatrix}$, $b=\begin{pmatrix}  4\\ 3 \\-3\end{pmatrix}$ , $x=\begin{pmatrix}  x_1\\ x_2 \\x_3\end{pmatrix}$\


$$A^{-1}=\begin{pmatrix} \frac{3}{2}&   \frac{1}{2} &   -\frac{1}{6}\\ -\frac{3}{4} &   \frac{1}{4} &   \frac{1}{4} \\ 1 &   0 &   -\frac{1}{3} \end{pmatrix}$$\

\hspace{3cm}

Solution: $x=A^{-1}b=\begin{pmatrix} \frac{3}{2}&   \frac{1}{2} &   -\frac{1}{6}\\ -\frac{3}{4} &   \frac{1}{4} &   \frac{1}{4} \\ 1 &   0 &   -\frac{1}{3} \end{pmatrix}$$\begin{pmatrix} 4\\ 3\\ -3 \end{pmatrix}=\begin{pmatrix} 5\\ -3\\ 5 \end{pmatrix}$\

\newpage

\begingroup
 
\begin{tabular}{ c c c c c c }
$\bold{Step 1}$ &  & $\bold{Step 2}$  &  & $\bold{Step 3}$ \\
$\begin{vmatrix} 1 & 2 & 1\\ 0 & 4 & 3 \\ 3 & 6 & 0 \end{vmatrix}$ &  & $\begin{vmatrix} 1 & 0 & 1\\ 0 & 4 & 3 \\ 3 & 0 & 0 \end{vmatrix}$ &  & $\begin{vmatrix} 1 & 0 & 0\\ 0 & 4 & 3 \\ 3 & 0 & -3 \end{vmatrix}$ \\  
 & $\xrightarrow{-2*C1 + C2}$ &  & $\xrightarrow{-1C1*C3}$ & & $\xrightarrow{-3*C2}$\\
$\begin{vmatrix} 1 &   0 &   0\\ 0 &   1 &   0 \\ 0 &   0 &   1 \end{vmatrix}$ &  & $\begin{vmatrix} 1 &   -2 &   0\\ 0 &   1 &   0 \\ 0 &   0 &   1 \end{vmatrix}$ &  & $\begin{vmatrix} 1 &   -2 &   -1\\ 0 &   1 &   0 \\ 0 &   0 &   1 \end{vmatrix}$\\
 &  &  \\
\end{tabular}
\endgroup

\hspace{3cm}

\begingroup
 
\begin{tabular}{ c c c c c c }
$\bold{Step 4}$ &  & $\bold{Step 5}$  &  & $\bold{Step 6}$ \\
$\begin{vmatrix} 1 & 0 & 0\\ 0 & -12 & 3 \\ 3 & 0 & -3 \end{vmatrix}$ &  & $\begin{vmatrix} 1 & 0 & 0\\ 0 & -12 & 12 \\ 3 & 0 & -12 \end{vmatrix}$ &  & $\begin{vmatrix} 1 & 0 & 0\\ 0 & -12 & 0 \\ 3 & 0 & -12 \end{vmatrix}$ \\  
 & $\xrightarrow{4*C3}$ &  & $\xrightarrow{C2+C3}$ & & $\xrightarrow{4*C1}$\\
$\begin{vmatrix} 1 &   6 &   -1\\ 0 &   -3 &  0 \\ 0 &   0 &   1 \end{vmatrix}$ &  & $\begin{vmatrix} 1 &   6 &   -4\\ 0 &   -3 &   0 \\ 0 &   0 &   4 \end{vmatrix}$ &  & $\begin{vmatrix} 1 &   6 &   2\\ 0 &   -3 &   -3\\ 0 &   0 &   4 \end{vmatrix}$\\
 &  &  \\
\end{tabular}
\endgroup

\hspace{3cm}

\begingroup
 
\begin{tabular}{ c c c c c c }
$\bold{Step 7}$ &  & $\bold{Step 8}$  &  & $\bold{Step 9}$ \\
$\begin{vmatrix} 4 & 0 & 0\\ 0 & -12 & 0 \\ 12 & 0 & -12 \end{vmatrix}$ &  & $\begin{vmatrix} 4 & 0 & 0\\ 0 & -12 & 0 \\ 0 & 0 & -12 \end{vmatrix}$ &  & $\begin{vmatrix} 1 & 0 & 0\\ 0 & -12 & 0 \\ 0 & 0 & -12 \end{vmatrix}$ \\  
 & $\xrightarrow{C3+C1}$ &  & $\xrightarrow{1/4*C1}$ & & $\xrightarrow{-1/12*C2}$\\
$\begin{vmatrix} 4 &   6 &   2\\ 0 &   -3 &  -3 \\ 0 &   0 &   4 \end{vmatrix}$ &  & $\begin{vmatrix} 6 &   6 &   2\\ -3 &   -3 &   -3 \\ 4 &   0 &   4 \end{vmatrix}$ &  & $\begin{vmatrix} 1.5 &   6 &   2\\ -0.75 &   -3 &   -3\\ 1 &   0 &   4 \end{vmatrix}$\\
 &  &  \\
\end{tabular}
\endgroup

\hspace{3cm}

\begingroup
 
\begin{tabular}{ c c c  }
$\bold{Step 10}$ &  & $\bold{Step 11}$\\
 $\begin{vmatrix} 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & -12 \end{vmatrix}$ &  & $\begin{vmatrix} 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{vmatrix}$ \\  
 & $\xrightarrow{-1/12*C2}$\\
 $\begin{vmatrix} 1.5 & -0.5 &   2\\ -0.75 & 0.25 &  -3 \\ 1 &   0 &   4 \end{vmatrix}$ &  & $\begin{vmatrix} 1.5 & -0.5 & -0.17\\ -0.75 & 0.25 & 0.25 \\ 1 & 0 & -0.33\end{vmatrix}$\\
 &  &  \\
\end{tabular}
\endgroup

\newpage

#9. EIGENVALUES AND EIGENVECTORS\


Remember that $A_{nxm}$, represents a linear map $R^{n}$ ---> $R^{n}$.\

Consider the question: Are there any vectors $\underline{x}$ $\epsilon$ $R^{n}$ s.t. A, either streches or shrinks them?\

That is there $\lambda$ $\epsilon$ $R$ and $\underline{x}$ $\epsilon$ $R^{n}$ s.t.\

\hspace{3cm}

 $(*)$ A*$\underline{x}$ = $\lambda$ $\underline{x}$\
 
The case $\underline{x}$ = $\underline{0}$ always works and is not interesting.\

If such $\lambda$ and $\underline{x}$ $\not=$ 0 fixits:

$\lambda$ - EIGENVALUE\

\hspace{1cm}

$\underline{x}$ - EIGENVECTOR\

\hspace{1cm}

We can rewrite $(*)$:\

\hspace{1cm}

A*$\underline{x}$ - $\lambda$ I $\underline{x}$ = $\underline{0}$\

(A-$\lambda$ I) $\underline{x}$ = $\underline{0}$\

since:

$\underline{x}$ $\not=$ $\underline{0}$ and,\

(A-$\lambda$ I) must be singular\

$(**)$ |A - $\lambda$I| = 0  is called CHARACTERISTIC EQUATION and is a polynomial of degree n in $\lambda$. Which may or may not have real roots, but always has COMPLEX roots.\

One can solve for the roots of the polynomial to find the eigenvalues.

Note that if $\underline{u}$ is an eigenvector then so is $\alpha$$\underline{u}$ for $all$ $\alpha$ $\not=$ 0.

####EXAMPLE (calculating eigenvalues)\

$$A =\begin{pmatrix} 1 & 4 \\ 9 & 1\end{pmatrix}$$\

\hspace{3cm}

We solve the characteristic equation:\

\hspace{3cm}

(A-$\lambda$ I) = |$\begin{pmatrix} 1 & 4 \\ 9 & 1\end{pmatrix}$ - $\begin{pmatrix} \lambda & 0 \\ 0 & \lambda\end{pmatrix}$| = |$\begin{pmatrix} 1-\lambda & 4 \\ 9 & 1-\lambda\end{pmatrix}$| = 0\



$\Longleftrightarrow$ $(1-\lambda)^2$ - 4*9=0\

\hspace{3cm}

$\Longleftrightarrow$ $\lambda_{1}$=-5,  $\lambda_{2}$=7\

\newpage

####EXAMPLE: Find Eigenvectors\

$\lambda_{1}$=-5

A-$\lambda_{1}$I = $\begin{pmatrix}  6 & 4\\ 9 & 1\end{pmatrix}$

\hspace{2cm}

$\begin{pmatrix}  6 & 4\\ 9 & 1\end{pmatrix}\begin{pmatrix}  x_{1}\\ x_{2}\end{pmatrix}$ = 0 $\Longleftrightarrow$ $6x_{1} + 4x_{2} = 0$\

\hspace{2cm}

One solution is: $\begin{pmatrix}  2 \\ -3\end{pmatrix}$

$\lambda_{2}$=7

A-$\lambda_{2}$I = $\begin{pmatrix}  -6 & 4\\ 9 & -6\end{pmatrix}$

\hspace{2cm}

$\begin{pmatrix}  -6 & 4\\ 9 & -6\end{pmatrix}\begin{pmatrix}  x_{1}\\ x_{2}\end{pmatrix}$ = 0 $\Longleftrightarrow$ $-6x_{1} + 4x_{2} = 0$\

\hspace{2cm}

One solution is: $\begin{pmatrix}  2 \\ 3\end{pmatrix}$

\hspace{3cm}

####PROPERTIES OF EIGENVALUES:

i) Eigenvalues can be complex for ...
ii) If A is symmetric --> real eigenvalues
iii) |A|= $\prod\limits_{i=1}^{n} \lambda{i}$ 
iv) $T_{r}$(A)=$\sum\limits_{i=1}^{n} \lambda{i}$
v) Rank of A = number of nonzero eigenvalues (Counting Multiple Roots)
vi) $\exists A^{-1} \Longleftrightarrow$ all eigenvalues are non zero
vii) If A is symmetric then $\exists$ n LIN eigenvectors (one for $\forall$ root and multiple vectors for multiple roots)

\newpage

####EXAMPLE: Find eigenvalues and eigenvectors of the following symmetric matrix:\


$$A=\begin{pmatrix}  1 & -1 & 0\\ -1 & 1 & 0\\ 0 & 0 & 2\end{pmatrix}$$

|A-$\lambda$ I| = $\begin{vmatrix} 1-\lambda & -1 & 4 \\ -1 & 1-\lambda & 0 \\ 0 & 0 & 2-\lambda\end{vmatrix}$ 

\begin{align}
     &=(1-\lambda)^2(2-\lambda)-(-1)(-1)(2-\lambda) \\
     &= (1-\lambda)^2(2-\lambda)-(2-\lambda)\\
     &=(2-\lambda)((1-\lambda)^2-1) \\
     &=(2-\lambda)(1-\lambda -1)(1-\lambda +1)\\
     &=-(2-\lambda)^2\lambda
\end{align}


Hence, the eigenvalues are: 0, 2, 2 (multiple)\


Eigenvectors:

\hspace{2cm}


 
\begin{tikzcd}
  0 \arrow[row sep=tiny]{r} & \begin{pmatrix}  -1 \\  1 \\ 0 \end{pmatrix}
\end{tikzcd}



\hspace{2cm}

\begin{tikzcd}[row sep=tiny]
                         & \begin{pmatrix}  -1 \\  1 \\ 0 \end{pmatrix}              \\
2 \arrow{ur}\arrow{dr} &                  \\ 
                          & \begin{pmatrix}  0 \\  0 \\ 1 \end{pmatrix}
\end{tikzcd}


One can check that 3 eigenvectors are LIN, moreover there are orthogonal.






















































  





  
  
  







  
  
    



      





      


  
  
    
    
    
  
  
  
  
    
          

 


  
  
  






  



















 
           
  
  
  

 



  



  
  

  

    
 












    


                          
                            



