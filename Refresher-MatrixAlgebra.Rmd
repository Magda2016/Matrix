---
title: "Matrix Algebra - Refresher"
author: "T. Markaryan"
date: "May 11, 2016"
output: 
  pdf_document:
    includes:
      in_header: MatrixLatexLibraries.sty
---

A special value of matrices and Matrix Algebra is that they nable to express many operations arising in mathematics, statistics and many other fields in a clear concise manner. Matrices also enhance the understanding of which apsects generalize to higher dimensions.


#1. Matrices as Transformations

####$R \to R$ 

$$y=ax$$

A single number $\underline{a}$ represents linear transformation.

####$R^2 \to R$ 

$$y=a_1x_1+a_2x_2$$

Vector $\mathbf{A}=(a_1, a_2)$ represents linear transformation.

####$R^2 \to R^2$ 


$$\begin{cases} y_1 = a_{11}x_1 + a_{12}x_2\\ y_2 = a_{21}x_1 + a_{22}x_2 \end{cases}$$

Matrix  $\begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}$ represents linear transformation.

\begin{tcolorbox}[colback=yellow!5,colframe=yellow!40!black,title=Questions]
1. What matrix corresponds to a linear mapping?

2. What matrix corresponds to a linear maping $R^{10} \to R^{20}?$
\end{tcolorbox}

Thus, for linear mappings:

- $R \to R$  | given by numbers

- $R^{n} \to R^{m}$  | given by matrices


So, we can write compactly

 \begin{equation}\label{eqn:matrix}\underline{y}=A\underline{x}\end{equation}
 
 where
 
 $$\underline{y}=\binom{y_1}{y_2},  A = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} , \underline{x}=\binom{x_1}{x_2}$$


Note that equation \eqref{eqn:matrix} looks similar to the linear equation in the case of $R \to R$

In general, for transformation $R^{n} \to R^{m}$, a matrix  $$\begin{pmatrix}A\end{pmatrix}_{m*n}$$ reprezents a linear transformation. 

Viewing matrices as linear transformations can help understand, seemingly strange definition of a product of matrices as well as:

- Addition of matrices

- Multiplication by a scalar

- Inverse 




#2. Notation and Definitions\



\begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
A matrix is an $m*n$ array of numbers with  $\underline{m}$ rows and  $\underline{n}$ columns.
\end{tcolorbox}
\    



Matrices are usually denoted by capital's: A,B, C...


Generic elemment of matrix is written as:

\begin{center}
$\begin{pmatrix}A\end{pmatrix}_{ij}$ or $a_{ij}$, where  $i=1,...,m ; j=1,...n$
\end{center}
\






Examples of Matrices:

  - $\mathbf{1x1}$ marix is a $\underline{scalar}$
  
  - $\mathbf{nx1}$ marix is a $\underline{column vector}$
  
  - $\mathbf{1xn}$ marix is a $\underline{row vector}$
  
  - $\mathbf{nxn}$ marix is a $\underline{square matrix}$


We denote column vectors by $a, b, z...$ and to emphasize that they are vectors we use:\

$\underline{a}, \underline{b}, \underline{z}$.\






$\mathbf{Equality}$ of matrices: $A=B$ 

  - They are the same size
  - $A_{ij}=B_{ij}$
  
  
$\mathbf{Transpose}$ of matrix:

   - If $A$ is a $m*n$ matrix, then $(A')_{ij}$ = $(A)_{ij}$ is a $n$x$m$ matrix.
\


Examples of transpose will be provide.\



$\mathbf{Symmetric}$ $A'=B$ \





#3. Matrices as Transformations


####Addition of Matrices\





\begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
If $A$ and $B$ are $mxn$ matrices then $C=A+B$ is defined as elementwise addition.

$(C)_{ij}$ = $(A)_{ij}$ + $(B)_{ij}$ 	   $\forall$            ${i,j}$
\end{tcolorbox}
\  





$\mathbf{Examples}$ \




a) A= $\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$ 
  B= $\begin{pmatrix} -1 & 1 \\ 1 & -4 \end{pmatrix}$;
  A+B=$\begin{pmatrix} 1+(-1) & 2+1 \\ 3+1 & 4+(-4) \end{pmatrix}$
  \
  
  
  
  
  
  
  
  b)  A= $\begin{pmatrix} 1 & 3 & 5 \\ 2 & 4 & 6\end{pmatrix}$ 
      B= $\begin{pmatrix} -1 & -3 & -5 \\ -2 & -4 & -6\end{pmatrix}$; A+B=$\begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 0\end{pmatrix}$\
      
      
      
      
      
$\textbf{Additive Identities }$\







Matrix  $O_{mxn}$  consisting of all $O's$ is called $\textbf{Null Matrix}$ and serves as the additive identitiy.\




$$A_{mxn}+O_{mxn}=A_{mxn}$$\



-> $\text{\textit{Note: There is an additive identity for each size}}$\






$\textbf{Properties of Matrix Addition}$\





- Commutative: $A+B = B+A$\

- Associative: $A+(B+C) = (A+B)+C$    $\text{\textit{Why?}}$\

- Multiplication by a scallar: If $A = A_{ixj}$ then $B=cA=(c*A_{ixj})$

$\mathbf{Example}$ \







4*$\begin{pmatrix} 1 & 0 & -1 \\ 2 & 1 & 4\end{pmatrix}$ = $\begin{pmatrix} 4 & 0 & -4 \\ 8 & 4 & 16\end{pmatrix}$\




#4. Multiplication of Matrices\



Matrix multiplication may first seem unnatural or strange, but viewing it as a $\textbf{composition}$ of two linear maps makes it completelly clear.\


$\mathbf{Example}$ \

  - One-dimensional case
  
  $$\mathbbm{R}\xrightarrow{--1--}\mathbbm{R}\xrightarrow{--2--}\mathbbm{R}$$
  $$\overrightarrow{---3---}$$\
  
  
  1: $y=\alpha*x$\
  
  2:  $\zeta=\beta*y$\
  
  3:$\zeta=\beta*\alpha*x$
  
  
  
  
   -  $\mathbbm{R^2}$  case
   
   
  
  $$\mathbbm{R^2}\xrightarrow{--1--}\mathbbm{R^2}\xrightarrow{--2--}\mathbbm{R^2}$$
  $$\overrightarrow{---3---}$$\
  
  
  1:$\begin{cases} y_1 = x_1 + x_2\\ y_2 = x_1 - x_2 \end{cases}$\
  
  and, 
  
  \begin{equation}\label{eqn:matrix}\underline{y}=A\underline{x}\end{equation}
  
   $$A = \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}$$
  
  
  
  2:  $\begin{cases} \zeta_1= y_1 + 2y_2\\ \zeta_2 = y_1 - y_2 \end{cases}$\
  
  
  
  and,
  
  \begin{equation}\label{eqn:matrix}\underline{\zeta}=B\underline{y}\end{equation}
  
   $$B = \begin{pmatrix} 1 & 2 \\ 1 & -1 \end{pmatrix}$$\
   
   
   
  
  3:$\begin{cases} \zeta_1= x_1 + x_2 + 2x_1 - 2x_2 \\ \zeta_2 = x_1 + x_2 - x_1 + x_2 \end{cases}$ 	$\Longleftrightarrow$   $\begin{cases} \zeta_1= 3x_1 - x_2\\ \zeta_2 = 2x_2 \end{cases}$\
  
  
  thus, 
  
  
  
  \begin{equation}\label{eqn:matrix}\underline{\zeta}=C\underline{x}\end{equation}
  
   $$C = \begin{pmatrix} 3 & -1 \\ 0 & 2 \end{pmatrix}$$\
   
   
So it is natural to define $BA=C$, and we can easily verify that:

 $$BA = \begin{pmatrix} 1 & 2 \\ 1 & -1 \end{pmatrix}\begin{pmatrix} 1 & 1\\ 1 & -1 \end{pmatrix}=\begin{pmatrix} 3 & -1 \\ 0 & 2 \end{pmatrix}$$\
 
 
 
 Identifying multiplication as a composition should make it natural that matrix mulitplication is only defined for matrices:\
 
 
 $$\begin{pmatrix}A\end{pmatrix}_{m*n} \begin{pmatrix}B\end{pmatrix}_{n*p}$$
 
 $$n=m$$\
 
 -> Dimensions must conform before $A*B$ can be defined.\
 
   $$\mathbbm{R_p}\xrightarrow{--B--}\mathbbm{R_n}\xrightarrow{--A--}\mathbbm{R_m}$$
  $$\overrightarrow{---AB---}$$\
  
  
  \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
Let $\begin{pmatrix}A\end{pmatrix}_{m*n}$ and $\begin{pmatrix}B\end{pmatrix}_{n*p}$  then $C=AB$ is defined
as $\begin{pmatrix}C\end{pmatrix}_{i*j} = \sum\limits_{k=1}^n \mathcal A_{ik}B_{kj}$ ; i=1,...,m; j=1,...,p
\end{tcolorbox}
\ 



$\mathbf{Examples}$ \


a)  $\begin{pmatrix} 3 & 1 & 10 \\ 2 & 2 & 8 \end{pmatrix}$ 
   $\begin{pmatrix} 60 \\ 80 \\ 40 \end{pmatrix}$ = $\begin{pmatrix} 660 \\ 600  \end{pmatrix}$\
   
   
   
  -> The matrices are represented by the dimensions as follows: 2x3, 3x1 and 2x1.
  \
  
  
b)  $\begin{pmatrix} 1 & 2 \\ 3 & 4  \end{pmatrix}$ 
   $\begin{pmatrix} 0 & -1 \\ 1 & -1  \end{pmatrix}$ = $\begin{pmatrix} 2 & -3 \\ 4 & -7  \end{pmatrix}$\
   
   
   
  -> The matrices are represented by the dimensions as follows: 2x2, 2x2 and 2x2.
  \
  
    
c)  $\begin{pmatrix} 0 & -1 \\ 1 & -1  \end{pmatrix}$ 
   $\begin{pmatrix} 1 & 2 \\ 3 & 4  \end{pmatrix}$ = $\begin{pmatrix} -3 & -4 \\ -2 & -2  \end{pmatrix}$\
   
   
   
d)  Inner Product\

      $\begin{pmatrix} 2 & 2 & 8 \end{pmatrix}$ 
   $\begin{pmatrix} 50 \\ 100 \\ 30  \end{pmatrix}$ = $540$\
   
-> The matrices are represented by the dimensions as follows: 1x3, 3x1 and 1x1.
  \
  
  
More generally, if $\begin{pmatrix} x_i \\ .\\ .\\.\\x_n  \end{pmatrix}$ $\begin{pmatrix} y_i \\ .\\ .\\.\\y_n  \end{pmatrix}$,  then  $X'Y=Y'X = \sum\limits_{i=1}^n \mathcal X_{i}Y_{i}$


e)  Outer Product\


    $\begin{pmatrix} 50 \\ 100 \\ 30  \end{pmatrix}$ $\begin{pmatrix} 2 & 2 & 8 \end{pmatrix}$ = $\begin{pmatrix} 100 & 100 & 400 \\ 200 & 200 & 800 \\ 60 & 60 & 240 \end{pmatrix}$\
   
   
   
   
-> The matrices are represented by the dimensions as follows: 3x1, 1x3 and 3x3.
  \
 
 
 
-> $\text{\textit{Note: b) $\neq$ c) which means that $\textbf{matrix mulitplication}$ is  $\textbf{NOT}$ commutative. Examples d) and e) confirm that.}}$\
   
  \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition (Linear Forms)]
If if $\underline{x}=\begin{pmatrix} x_i \\ .\\ .\\.\\x_n  \end{pmatrix}$ is a vector of variables $x_1, ...x_n$ and $\underline{a}=if \begin{pmatrix} a_i \\ .\\ .\\.\\a_n  \end{pmatrix}$ is a vector of constants than $a'x=a_1x_1+a_2x_2+...a_nx_n$ is called a $\textbf{linear form}$ in x.



\end{tcolorbox}\







It is easy to show that:

$$\frac{d}{dx}(a'x)=\underline{a}$$\





 \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition (Length of a vector)]
If if $\underline{x}=\begin{pmatrix} x_i \\ .\\ .\\.\\x_n  \end{pmatrix}$ $\in \mathbbm{R^n}$ then, $\|x\|$=$\sqrt{x'x}$=$\sqrt{\sum\limits_{i=1}^n x^2_i}$, if $\|x\|$=1, then x has an unit length.




\end{tcolorbox}\




$\textbf{Quadratic Forms}$.

If A is symmetric then $\underline{x'}A\underline{x}$ is called a $\textbf{quadratic form}$\


-> $\text{\textit{Note: $x'Ax$ ia a scalar.}}$\



$\mathbf{Examples}$ \



$\begin{pmatrix} x_1 & x_2 \end{pmatrix}$ 
   $\begin{pmatrix} 1 & 3 \\ 3 & 2 \end{pmatrix}$ $\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$ = ${x^{2}_1 + 6x_1x_2 + 2x^{2}_2}$\ 
                                  ->$\text{\text{(quadratic form in 2 variables)}}$\
                                  
                                  
                                  
                                  
                                  
                                  
                                  
                                  
  -In 1-dimensional case: $x*a*x = ax^2$ & $\frac{d}{dx}(x*a*x)=2ax$\


  -In higher dimensions: $\frac{d}{dx}(\underline{x'}*A*\underline{x})=2A\underline{x}$ -> $\text{\textit{Show it!}}$\
  
  
  
  
  
$\textbf{Definite and semi-definite matrices.}$\
  
  
  
  \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
A symmetric matrix $\underline{A}$ is said to be $\textbf{positive definite}$ if $\underline{x'}A\underline{x}$  $>$ for all $\underline{x}\neq0$



\end{tcolorbox}\



  -Positive semi-definite is defined by changing $">"$ to $"\geq"$.
  
  -Negative definite and negative semi-definite are defined using $"<"$ and $"\leq"$, respectively.\
  
  
  
  
$\mathbf{Example}$\



  -Variance-Covariance Matrices are Positive Semi-Definite.\
  
  
  
  
  
$\mathbf{Example}$\


a) Show that:\

$A=\begin{pmatrix} 2 & -2 \\ -2 & 2 \end{pmatrix}$ is positive semi-definite.\


b) Show that:\

$B=\begin{pmatrix} 4 & -2 & 0\\ -2 & 4 & -2 \\ 0 & -2 & 4\end{pmatrix}$ is positive definite.\


c) Show that:\

$C=\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}$ is indefinite.\



  
$\textbf{Properties of Matrix Multiplication.}$\



  -1. Associative:\
  
    $A(BC)=(AB)C$\
  
  -2. Distributive:\
  
    $A(B+C)=AB+AC$\
    
    $(B+C)A=BA+CA$\
    
  -3. NOT-Commutative\
  
  -4. Identity Matrix\
  
  $I_{nxn}$=$\begin{bmatrix} 1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 1 \end{bmatrix}$ serves as multiplicative identity for square matrices for $\forall$ $A_{nxn}$
  
  $$I_{nxn}A_{nxn}=A_{nxn}I_{nxn}=A_{nxn}$$\
  
  
  

-> $\text{\textit{Note:In case of identity matrix product does not commute!}}$\


-v5. For any matrix $A_{mxn}$\

$$A_{mxn}O_{nxp}=O_{mxp}$$\

$$(AB)'=B'A'$$
$$(ABC)'=C'B'A'$$\



#5. Inverse of Matrices\




  
  \begin{tcolorbox}[colback=green!5,colframe=red!40!black,title=Definition]
  If for a squarmatrix $A_{nxn}$ $\exists$  a matrix $B_{nxn}$ sit.\
  
  (*) $AB = BA = I_{nxn}$.  Than $B$ is calles an $\underline{inverse}$ of $A$ and denoted $A^-1$. If $A$ has an inverse, it is called $\underline{non-singular}$.
  

\end{tcolorbox}\


  -It turns out that one needs to check only one of the 2 conditions in (*).\
  
  -The inverse if it exists is $\underline{unique}$.\
  
  
  
  
$\mathbf{Example}$\

a)$A=\begin{pmatrix} 2 & -1 \\ -1 & 1  \end{pmatrix}$ then $A^{-1}=\begin{pmatrix} 1 & 1 \\ 1 & 2  \end{pmatrix}$  -> check directly.\


b)Show that $A=\begin{pmatrix} 1 & 3 \\ 2 & 6  \end{pmatrix}$ does not have an inverse.\


c)Show that if $A=\begin{pmatrix} a & b \\ c & d  \end{pmatrix}$ than $A^{-1}
=\frac{1}{ad-bc}\begin{pmatrix} d & -b \\ -c & a  \end{pmatrix}$ iff ad-bc $\neq$ 0.\


####Properties of Matrix Inverse\

  a) $(A')^{-1}=(A^{-1})'$\
  
  b) $(A^{-1})^{-1}=A$\
  
  c) $(AB)^{-1}=B^{-1}A^{-1}$\
  
  
####Proof of c)\

Need to show that:\

$$(AB)(B^{-1}A^{-1})= I$$

Now:

  
\begin{align}
    (AB)(B^{-1}A^{-1}) &=A(BB^{-1})A^{-1}&& \text{(Associative}  \text{)}\\
     &=AIA^{-1} && BB^{-1}=I \\
     &=AA^{-1}   && BB^{-1}=I \\
     &=I &&\text{(Definition of Inverse}  \text{)}
\end{align}

\


c) Also extends to more than 2 matrices. For example:\
$$(ABC)^{-1}=C^{-1}B^{-1}A^{-1}$$

----> Show it!\

  -Armed with definition of matrix inverse we can cast system of linear equations and solution in the matrix form.\
  
  
 (*) $$\begin{cases} x_1 + 2x_2 +x_3= 4\\ 4x_2 + 3x_3 = 3\\3x_1 + 6x_2 = -3 \end{cases}$$\
  
  Denote:\
  
  
$A=\begin{pmatrix} 1 & 2 & 1\\ 0 & 4 & 3 \\ 3 & 6 & 0\end{pmatrix}$, $b=\begin{pmatrix}  4\\ 3 \\-3\end{pmatrix}$ , $x=\begin{pmatrix}  x_1\\ x_2 \\x_3\end{pmatrix}$\




  We can write (*) as:\
  
  (**) $$A\underline{x}=\underline{b}$$
  
  Also, if (*) has an unique solution then:
  
  $$\underline{x}=A^{-1}b$$
  
  Which is how we solve in case of $\mathbbm{R^1}$.\
  
  
  - We will return and solve this equation after introducing column reduction.
  
  - As we have already seen, not every square matrix has an inverse. A <=> condition for existence of the inverse is for the columns of the matrix to be
 $\textbf{LINEARLY INDEPENDENT}$.\
 
 
 
####Linear Independence\

- Two vectors of same size:\

$a=\begin{pmatrix} a_1 \\ .\\ .\\.\\a_n  \end{pmatrix}$ & $b=\begin{pmatrix} b_1 \\ .\\ .\\.\\b_n  \end{pmatrix}$\


Are called $\textbf{LINEARLY INDEPENDENT}$ if for $\forall$   $\lambda_1$, $\lambda_2$, $\in$ $R$ siti $\lambda^{2}_1 + \lambda^{2}_2 > 0$







 
           
  
  
  

 



  



  
  

  

    
 












    


                          
                            



